{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "normal-council",
   "metadata": {},
   "source": [
    "### This script will scrape and process the Code Violations and Citations.\n",
    "#### Note: Do not browse the internet or have extra tabs open while the scrapper is running.\n",
    "#### Note: Do not run multiple copies of  the script at once. Copies of APIs do not play well when run at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "equal-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "import datetime as dt\n",
    "from geopy.geocoders import Nominatim\n",
    "import re\n",
    "import csv\n",
    "#import censusgeocode as cg \n",
    "import sys\n",
    "import glob\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "start_time = time.time()\n",
    "\n",
    "intermediate_sub_path = \"intermediate_files/\" #r\"intermediate_files//\"\n",
    "raw_sub_path = \"raw_files/\" #r\"intermediate_files//\"\n",
    "\n",
    "\n",
    "# NOTE: addressCorrections located here (Import_Shared_Functions.py in same folder as this script)\n",
    "#from Import_Shared_Functions import *\n",
    "# NOTE: RAW NBConvert blocks of code in this script below are only for debugging. Import above is all you need. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-browse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "occupied-matthew",
   "metadata": {},
   "source": [
    "### Baltimore City Code Violations and Citations Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "heard-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baltimore_city_scraper(citations_or_violations = \"violations\"):\n",
    "    print(\"You have started the %s scrapping tool.\"%citations_or_violations)\n",
    "    scrapperColumnsDict = {\"violations\":[\"Address\", \"Type\", \"Date_Notice\", \"Notice_Number\", \"District\", \"Neighborhood\"],\n",
    "                           \"citations\":[\"Photo\",\"Citation Number\", \"Description\", \"Address\", \"Issue Date\", \"District\", \"Neighborhood\"]}\n",
    "    \n",
    "    # generate scrapper_output data frame. Takes about 60 minutes.\n",
    "    scrapper_start_time = time.time()\n",
    "    scrapper_output = pd.DataFrame()\n",
    "    scrapper_output_index = 0\n",
    "    main_page = \"http://cels.baltimorehousing.org/Search_On_Map.aspx\"\n",
    "\n",
    "\n",
    "    # initialize chrome options, do not load images since we don't need them.\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    # Initiate the driver for chrome.\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
    "    # driver = webdriver.Chrome(executable_path=\"C:\\Program Files (x86)\\chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "    # make a get request from the page.\n",
    "    driver.get(main_page)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # get city names from dropdown menu.\n",
    "    cities = []\n",
    "    page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    city_node = page_soup.find_all(\"select\", {\"id\": \"ctl00_ContentPlaceHolder1_lstLoc\"})[0]\n",
    "    opts = city_node.find_all(\"option\")\n",
    "    for opt in opts:\n",
    "        if opt.text.strip() != \"\":\n",
    "            cities.append(opt.text.strip())\n",
    "\n",
    "    city_index = 0\n",
    "    for city in cities:\n",
    "        try:\n",
    "            start_time_city = time.time()\n",
    "            city_index = city_index+1\n",
    "\n",
    "            # make a get request from the page.\n",
    "            driver.get(main_page)\n",
    "            time.sleep(1)\n",
    "\n",
    "            # select \"by neighbourhood\".\n",
    "            driver.find_element_by_id(\"ctl00_ContentPlaceHolder1_ck2\").click()\n",
    "\n",
    "            # select \"Violation\" or \"Citation\"\n",
    "            if citations_or_violations.lower() == \"violations\":\n",
    "                driver.find_element_by_id(\"ctl00_ContentPlaceHolder1_rbVC_0\").click()\n",
    "            elif citations_or_violations.lower() == \"citations\":\n",
    "                driver.find_element_by_id(\"ctl00_ContentPlaceHolder1_rbVC_1\").click()\n",
    "            else:\n",
    "                print(\"ERROR. YOU NEED  TO SELECT \\\"violations\\\" or \\\"citations\\\"\")\n",
    "                return\n",
    "\n",
    "            # select city.\n",
    "            driver.find_element_by_id(\"ctl00_ContentPlaceHolder1_lstLoc\").send_keys(city)\n",
    "\n",
    "            # click search.\n",
    "            driver.find_element_by_id(\"ctl00_ContentPlaceHolder1_btSearch\").click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Pass the HTML contents to Beautiful Soup for parsing.\n",
    "            page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            scrapper = page_soup.find_all(\"tbody\")[0].find_all(\"tr\")\n",
    "            for address in scrapper[2:]:\n",
    "                columns = address.find_all(\"td\")\n",
    "                for i, col in enumerate(scrapperColumnsDict[citations_or_violations]):\n",
    "                    scrapper_output.at[scrapper_output_index, col] = columns[i].text.strip()\n",
    "                scrapper_output_index = scrapper_output_index+1\n",
    "\n",
    "            # Print the number of neighborhoods completed and processing time for each one. \n",
    "            print(f'{city_index}/{len(cities)}', \"--- \", f'{len(scrapper_output)}', \"cumulative addresses. -----%s seconds ---\" % round((time.time() - start_time_city),2))\n",
    "\n",
    "        except:\n",
    "            print(f'error in {city}')\n",
    "\n",
    "    # generate scrapper_output excel file.\n",
    "    #scrapper_output.to_excel(\"BaltimoreCity_%s_raw.xlsx\"%citations_or_violations, index=False)\n",
    "\n",
    "    # quit from the web driver\n",
    "    driver.quit()\n",
    "    \n",
    "    # print the total processing time.\n",
    "    ProcessingTime = round((time.time() - scrapper_start_time)/60,2)\n",
    "    print(\"Code %s Processing Time %s minutes\" % (citations_or_violations, ProcessingTime))\n",
    "    \n",
    "    # Return the Dataframe\n",
    "    return scrapper_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acknowledged-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validAddressRows(df_VAR, addr_field):\n",
    "    # Read in Excel/CSV Data as a dataframe\n",
    "\n",
    "    \"\"\"This function will process the 'Address' column in the given DataFrame\n",
    "    1. drop any leading zeros\n",
    "    2. After dropping the lead zero, drop all rows whose address does not start with a number.\n",
    "\n",
    "\n",
    "    (DataFrame) -> (DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 0. Drop all nans\n",
    "    df_VAR = df_VAR[~df_VAR[addr_field].isnull()]\n",
    "    \n",
    "    # 1. drop any leading zeros (also strip any resulting whitespace)\n",
    "    df_VAR.loc[:,addr_field] = df_VAR[addr_field].str.lstrip('0').str.strip()\n",
    "    \n",
    "    # 2. drop all rows whose address does not start with a number.\n",
    "    df_VAR = df_VAR[df_VAR[addr_field].str[0].str.isdigit()]\n",
    "    \n",
    "    # 3. drop all rows whose address contains \"(Descriptive Address)\"\n",
    "    df_VAR = df_VAR[~df_VAR[addr_field].str.upper().str.contains((\"DESCRIPTIVE ADDRESS\"))]\n",
    "    \n",
    "    # 4. Consolidate duplicate spaces\n",
    "    df_VAR[addr_field] = df_VAR[addr_field].replace('\\s+', ' ', regex=True)\n",
    "    \n",
    "    # Return the dataframe\n",
    "    return df_VAR\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accredited-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recentDaysFilter(df_rmf, days_rmf, date_field_input, date_field_output, addr_field_output):\n",
    "    \"\"\"\n",
    "    Input: A dataframe with an 'Address' column, and a 'Date Notice' column or an 'Issue date' column\n",
    "    Output: The same dataframe with only the most recent 6 months worth of data, sorted by the (new) Date and Address columns. \n",
    "    \"\"\"\n",
    "    # Filter the merged dictionary to the most recent (days_rmf) number of days\n",
    "    df_rmf[date_field_output]= pd.to_datetime(df_rmf[date_field_input])\n",
    "    df_rmf = df_rmf.sort_values(by=[date_field_output, addr_field_output])\n",
    "    priorDate = df_rmf[date_field_output].max() - timedelta(days=days_rmf)\n",
    "    df_rmf = df_rmf.loc[(df_rmf[date_field_output]>priorDate),:] #(code confirmed correct with excel results)\n",
    "    return df_rmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hairy-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countDiffDayDuplicates(df_ddd, date_field, addr_field):\n",
    "    \"\"\"\n",
    "    Input: A dataframe with a 'Date' and 'Address' column\n",
    "    Output: Same dataframe that only keeps duplicate addresses (duplicates on the same day don't count as duplicates)\n",
    "    \"\"\"\n",
    "    df_ddd.drop_duplicates(subset=[date_field, addr_field], keep='last', inplace=True) # duplicates on the same day don't count\n",
    "    sd = df_ddd.groupby([addr_field]).agg({addr_field:'count'}).transpose().to_dict(orient='records')[0]\n",
    "    df_ddd['duplicates'] = df_ddd[addr_field].map(sd) \n",
    "    df_ddd = df_ddd[df_ddd.duplicates>=2]\n",
    "    return df_ddd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "obvious-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCitations_only(cnty_name, cits_df, cits_date_col):\n",
    "    \"\"\"\n",
    "    input: a citations df. Also label the date column. Assumes \"Address\" columns in dfs. \n",
    "    output: a processed citations df. \n",
    "    \n",
    "    Note: This function was only created in the event of scraping only the citations because the violations \n",
    "    part of the https://cels.baltimorehousing.org/Search_On_Map.aspx website are down. \n",
    "    If both citations and violations are available, use processCitationsViolations() instead. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Process Citations\n",
    "    cits_df = validAddressRows(cits_df, addr_field='Address')\n",
    "    cits_df = recentDaysFilter(cits_df, days_rmf=180, date_field_input=cits_date_col, date_field_output='Date',addr_field_output='Address')\n",
    "    cits_df = countDiffDayDuplicates(cits_df, date_field='Date',addr_field='Address')\n",
    "    #cits_df.to_excel(cnty_name+\"_citations_processed.xlsx\", index=False)\n",
    "    cits_df['Source'] = 'Citation'\n",
    "    cits_df = cits_df[['Address', 'Date','Source']]\n",
    "    \n",
    "    mergedDf = cits_df\n",
    "    mergedDf['tempAddress'] = mergedDf['Address'].str[0:12]\n",
    "    mergedDf.drop_duplicates(subset=['tempAddress'], keep='last', inplace=True) # DROP IF FIRST 12 CHARACTERS MATCH\n",
    "    mergedDf= mergedDf.drop(columns=['tempAddress'])\n",
    "    mergedDf.reset_index(drop=True, inplace=True)\n",
    "    return mergedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c73f116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-petroleum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "spatial-width",
   "metadata": {},
   "source": [
    "### Baltimore City Code Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa005cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#baltimore_city_scrapper_output_violations = baltimore_city_scraper(citations_or_violations =\"violations\")\n",
    "#baltimore_city_scrapper_output_violations.to_excel(raw_sub_path + \"BaltimoreCity_ViolationsCitations_Raw_Temp/\" +\"_BaltimoreCity_%s_raw.xlsx\"%\"violations\", index=False)\n",
    "baltimore_city_scrapper_output_citations = baltimore_city_scraper(citations_or_violations =\"citations\")\n",
    "baltimore_city_scrapper_output_citations.to_excel(raw_sub_path + \"BaltimoreCity_ViolationsCitations_Raw_Temp/\" +\"_BaltimoreCity_%s_raw.xlsx\"%\"citations\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a98866f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post processing lengths:    vio len: 3963 cits len: 3245\n",
      "Wall time: 35.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load 'BaltimoreCity_citations_raw.xlsx' \n",
    "citations_df_raw = pd.read_excel(raw_sub_path + \"BaltimoreCity_ViolationsCitations_Raw_Temp/\" + \"_\"  + \"BaltimoreCity_citations_raw.xlsx\")\n",
    "\n",
    "# Process the most recent 6 months of violation, citation data together into a single file\n",
    "county_name = \"Baltimore_City\"\n",
    "\n",
    "# Process the raw citations data (valid address check, ciation within last 180 days, properties with 2 or more citations filter, column prep for propstream import )\n",
    "baltimore_city_citations_df_processed = processCitations_only(county_name, citations_df_raw, \"Issue Date\")\n",
    "\n",
    "# Save the processed baltimore city citations in the 'BaltimoreCity_ViolationsCitations_Processed' folder. \n",
    "baltimore_city_citations_df_processed.to_excel(raw_sub_path + \"BaltimoreCity_ViolationsCitations_Processed/\" +\"_BaltimoreCity_%s_processed.xlsx\"%\"citations\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016a8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cloudy-lighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 252 ms\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-story",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-circuit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-dance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-naples",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-national",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
