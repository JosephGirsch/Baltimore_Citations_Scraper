{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "normal-council",
   "metadata": {},
   "source": [
    "### This script will scrape and process the Code Violations and Citations.\n",
    "#### Note: Do not browse the internet or have extra tabs open while the scrapper is running.\n",
    "#### Note: Do not run multiple copies of  the script at once. Copies of APIs do not play well when run at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "equal-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "import datetime as dt\n",
    " \n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# NOTE: addressCorrections located here (Import_Shared_Functions.py in same folder as this script)\n",
    "#from Import_Shared_Functions import *\n",
    "# NOTE: RAW NBConvert blocks of code in this script below are only for debugging. Import above is all you need. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-browse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "occupied-matthew",
   "metadata": {},
   "source": [
    "### Baltimore City Citations Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "heard-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baltimore_city_scraper(citations_or_violations = \"violations\"):\n",
    "    print(\"You have started the %s scrapping tool.\"%citations_or_violations)\n",
    "    scrapperColumnsDict = {\"violations\":[\"Address\", \"Type\", \"Date_Notice\", \"Notice_Number\", \"District\", \"Neighborhood\"],\n",
    "                           \"citations\":[\"Photo\",\"Citation Number\", \"Description\", \"Address\", \"Issue Date\", \"District\", \"Neighborhood\"]}\n",
    "    \n",
    "    # generate scrapper_output data frame. Takes about 60 minutes.\n",
    "    scrapper_start_time = time.time()\n",
    "    scrapper_output = pd.DataFrame()\n",
    "    scrapper_output_index = 0\n",
    "    main_page = \"http://cels.baltimorehousing.org/Search_On_Map.aspx\"\n",
    "\n",
    "\n",
    "    # initialize chrome options, do not load images since we don't need them.\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    # Initiate the driver for chrome.\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
    "    # driver = webdriver.Chrome(executable_path=\"C:\\Program Files (x86)\\chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "    # make a get request from the page.\n",
    "    driver.get(main_page)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # get city names from dropdown menu.\n",
    "    cities = []\n",
    "    page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    city_node = page_soup.find_all(\"select\", {\"id\": \"ctl00_ContentPlaceHolder1_lstLoc\"})[0]\n",
    "    opts = city_node.find_all(\"option\")\n",
    "    for opt in opts:\n",
    "        if opt.text.strip() != \"\":\n",
    "            cities.append(opt.text.strip())\n",
    "\n",
    "    city_index = 0\n",
    "    for city in cities:\n",
    "        try:\n",
    "            start_time_city = time.time()\n",
    "            city_index = city_index+1\n",
    "\n",
    "            # make a get request from the page.\n",
    "            driver.get(main_page)\n",
    "            time.sleep(1)\n",
    "\n",
    "            # select \"by neighbourhood\".\n",
    "            driver.find_element_by_id(\"ctl00_ContentPlaceHolder1_ck2\").click()\n",
    "\n",
    "            # select \"Violation\" or \"Citation\"\n",
    "            if citations_or_violations.lower() == \"violations\":\n",
    "                driver.find_element_by_id(\"ctl00_ContentPlaceHolder1_rbVC_0\").click()\n",
    "            elif citations_or_violations.lower() == \"citations\":\n",
    "                driver.find_element_by_id(\"ctl00_ContentPlaceHolder1_rbVC_1\").click()\n",
    "            else:\n",
    "                print(\"ERROR. YOU NEED  TO SELECT \\\"violations\\\" or \\\"citations\\\"\")\n",
    "                return\n",
    "\n",
    "            # select city.\n",
    "            driver.find_element_by_id(\"ctl00_ContentPlaceHolder1_lstLoc\").send_keys(city)\n",
    "\n",
    "            # click search.\n",
    "            driver.find_element_by_id(\"ctl00_ContentPlaceHolder1_btSearch\").click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Pass the HTML contents to Beautiful Soup for parsing.\n",
    "            page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            scrapper = page_soup.find_all(\"tbody\")[0].find_all(\"tr\")\n",
    "            for address in scrapper[2:]:\n",
    "                columns = address.find_all(\"td\")\n",
    "                for i, col in enumerate(scrapperColumnsDict[citations_or_violations]):\n",
    "                    scrapper_output.at[scrapper_output_index, col] = columns[i].text.strip()\n",
    "                scrapper_output_index = scrapper_output_index+1\n",
    "\n",
    "            # Print the number of neighborhoods completed and processing time for each one. \n",
    "            print(f'{city_index}/{len(cities)}', \"--- \", f'{len(scrapper_output)}', \"cumulative addresses. -----%s seconds ---\" % round((time.time() - start_time_city),2))\n",
    "\n",
    "        except:\n",
    "            print(f'error in {city}')\n",
    "\n",
    "    # generate scrapper_output excel file.\n",
    "    #scrapper_output.to_excel(\"BaltimoreCity_%s_raw.xlsx\"%citations_or_violations, index=False)\n",
    "\n",
    "    # quit from the web driver\n",
    "    driver.quit()\n",
    "    \n",
    "    # print the total processing time.\n",
    "    ProcessingTime = round((time.time() - scrapper_start_time)/60,2)\n",
    "    print(\"Code %s Processing Time %s minutes\" % (citations_or_violations, ProcessingTime))\n",
    "    \n",
    "    # Return the Dataframe\n",
    "    return scrapper_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acknowledged-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validAddressRows(df_VAR, addr_field):\n",
    "    # Read in Excel/CSV Data as a dataframe\n",
    "\n",
    "    \"\"\"This function will process the 'Address' column in the given DataFrame\n",
    "    1. drop any leading zeros\n",
    "    2. After dropping the lead zero, drop all rows whose address does not start with a number.\n",
    "\n",
    "\n",
    "    (DataFrame) -> (DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 0. Drop all nans\n",
    "    df_VAR = df_VAR[~df_VAR[addr_field].isnull()]\n",
    "    \n",
    "    # 1. drop any leading zeros (also strip any resulting whitespace)\n",
    "    df_VAR.loc[:,addr_field] = df_VAR[addr_field].str.lstrip('0').str.strip()\n",
    "    \n",
    "    # 2. drop all rows whose address does not start with a number.\n",
    "    df_VAR = df_VAR[df_VAR[addr_field].str[0].str.isdigit()]\n",
    "    \n",
    "    # 3. drop all rows whose address contains \"(Descriptive Address)\"\n",
    "    df_VAR = df_VAR[~df_VAR[addr_field].str.upper().str.contains((\"DESCRIPTIVE ADDRESS\"))]\n",
    "    \n",
    "    # 4. Consolidate duplicate spaces\n",
    "    df_VAR[addr_field] = df_VAR[addr_field].replace('\\s+', ' ', regex=True)\n",
    "    \n",
    "    # Return the dataframe\n",
    "    return df_VAR\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accredited-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recentDaysFilter(df_rmf, days_rmf, date_field_input, date_field_output, addr_field_output):\n",
    "    \"\"\"\n",
    "    Input: A dataframe with an 'Address' column, and a 'Date Notice' column or an 'Issue date' column\n",
    "    Output: The same dataframe with only the most recent 6 months worth of data, sorted by the (new) Date and Address columns. \n",
    "    \"\"\"\n",
    "    # Filter the merged dictionary to the most recent (days_rmf) number of days\n",
    "    df_rmf[date_field_output]= pd.to_datetime(df_rmf[date_field_input])\n",
    "    df_rmf = df_rmf.sort_values(by=[date_field_output, addr_field_output])\n",
    "    priorDate = df_rmf[date_field_output].max() - timedelta(days=days_rmf)\n",
    "    df_rmf = df_rmf.loc[(df_rmf[date_field_output]>priorDate),:] #(code confirmed correct with excel results)\n",
    "    return df_rmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hairy-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countDiffDayDuplicates(df_ddd, date_field, addr_field):\n",
    "    \"\"\"\n",
    "    Input: A dataframe with a 'Date' and 'Address' column\n",
    "    Output: Same dataframe that only keeps duplicate addresses (duplicates on the same day don't count as duplicates)\n",
    "    \"\"\"\n",
    "    df_ddd.drop_duplicates(subset=[date_field, addr_field], keep='last', inplace=True) # duplicates on the same day don't count\n",
    "    sd = df_ddd.groupby([addr_field]).agg({addr_field:'count'}).transpose().to_dict(orient='records')[0]\n",
    "    df_ddd['duplicates'] = df_ddd[addr_field].map(sd) \n",
    "    df_ddd = df_ddd[df_ddd.duplicates>=2]\n",
    "    return df_ddd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "obvious-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCitations_only(cnty_name, cits_df, cits_date_col):\n",
    "    \"\"\"\n",
    "    input: a citations df. Also label the date column. Assumes \"Address\" columns in dfs. \n",
    "    output: a processed citations df. \n",
    "    \n",
    "    Note: This function was only created in the event of scraping only the citations because the violations \n",
    "    part of the https://cels.baltimorehousing.org/Search_On_Map.aspx website are down. \n",
    "    If both citations and violations are available, use processCitationsViolations() instead. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Process Citations\n",
    "    cits_df = validAddressRows(cits_df, addr_field='Address')\n",
    "    cits_df = recentDaysFilter(cits_df, days_rmf=180, date_field_input=cits_date_col, date_field_output='Date',addr_field_output='Address')\n",
    "    cits_df = countDiffDayDuplicates(cits_df, date_field='Date',addr_field='Address')\n",
    "    #cits_df.to_excel(cnty_name+\"_citations_processed.xlsx\", index=False)\n",
    "    cits_df['Source'] = 'Citation'\n",
    "    cits_df = cits_df[['Address', 'Date','Source']]\n",
    "    \n",
    "    mergedDf = cits_df\n",
    "    mergedDf['tempAddress'] = mergedDf['Address'].str[0:12]\n",
    "    mergedDf.drop_duplicates(subset=['tempAddress'], keep='last', inplace=True) # DROP IF FIRST 12 CHARACTERS MATCH\n",
    "    mergedDf= mergedDf.drop(columns=['tempAddress'])\n",
    "    mergedDf.reset_index(drop=True, inplace=True)\n",
    "    return mergedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c73f116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "concerned-petroleum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "spatial-width",
   "metadata": {},
   "source": [
    "### Baltimore City Code Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7628ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "baltimore_city_scrapper_output_citations = baltimore_city_scraper(citations_or_violations =\"citations\")\n",
    "baltimore_city_scrapper_output_citations.to_excel(\"BaltimoreCity_Citations_Raw_Temp/\" +\"_BaltimoreCity_%s_raw.xlsx\"%\"citations\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98866f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load 'BaltimoreCity_citations_raw.xlsx' \n",
    "citations_df_raw = pd.read_excel(\"BaltimoreCity_Citations_Raw_Temp/\" + \"_\"  + \"BaltimoreCity_citations_raw.xlsx\")\n",
    "\n",
    "# Process the most recent 6 months of violation, citation data together into a single file\n",
    "county_name = \"Baltimore_City\"\n",
    "\n",
    "# Process the raw citations data (valid address check, ciation within last 180 days, properties with 2 or more citations filter, column prep for propstream import )\n",
    "baltimore_city_citations_df_processed = processCitations_only(county_name, citations_df_raw, \"Issue Date\")\n",
    "\n",
    "# Save the processed baltimore city citations in the 'BaltimoreCity_Citations_Processed' folder. \n",
    "baltimore_city_citations_df_processed.to_excel( \"BaltimoreCity_Citations_Processed/\" +\"_BaltimoreCity_%s_processed.xlsx\"%\"citations\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016a8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-lighter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-story",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-circuit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-dance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-naples",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-national",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
